{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the JSON file\n",
    "input_data = pd.read_json('input.json')\n",
    "\n",
    "# Optionally, save as CSV if you need to reuse your CSV-based pipeline\n",
    "input_data.to_csv('input_2.csv', index=False)\n",
    "print(\"JSON converted to CSV: 'input.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# -------------------------------\n",
    "# Step 1: Load the Input CSV\n",
    "# -------------------------------\n",
    "# Replace 'input.csv' with your actual input file name\n",
    "input_data = pd.read_csv('input.csv')\n",
    "\n",
    "# -------------------------------\n",
    "# Step 2: Preprocess the Data\n",
    "# -------------------------------\n",
    "# If the input has a 'transaction_date', extract time features and drop the raw date\n",
    "if 'transaction_date' in input_data.columns:\n",
    "    input_data['transaction_date'] = pd.to_datetime(input_data['transaction_date'], errors='coerce')\n",
    "    input_data['year']  = input_data['transaction_date'].dt.year\n",
    "    input_data['month'] = input_data['transaction_date'].dt.month\n",
    "    input_data['day']   = input_data['transaction_date'].dt.day\n",
    "    input_data['hour']  = input_data['transaction_date'].dt.hour\n",
    "    input_data.drop(columns=['transaction_date'], inplace=True)\n",
    "\n",
    "# Define the expected features (must match training)\n",
    "expected_features = [\n",
    "    'transaction_amount',\n",
    "    'transaction_channel',\n",
    "    'transaction_payment_mode_anonymous',\n",
    "    'payment_gateway_bank_anonymous',\n",
    "    'payer_browser_anonymous',\n",
    "    'payer_email_anonymous',\n",
    "    'payee_ip_anonymous',\n",
    "    'payer_mobile_anonymous',\n",
    "    'year',\n",
    "    'month',\n",
    "    'day',\n",
    "    'hour'\n",
    "]\n",
    "\n",
    "# Ensure all expected features are present; if missing, add them as NaN\n",
    "for col in expected_features:\n",
    "    if col not in input_data.columns:\n",
    "        input_data[col] = np.nan\n",
    "\n",
    "# Impute missing values:\n",
    "# For numeric features, fill with the mean; for categorical features, fill with 'missing'\n",
    "for col in expected_features:\n",
    "    if input_data[col].dtype == 'object':\n",
    "        input_data[col] = input_data[col].fillna('missing')\n",
    "    else:\n",
    "        input_data[col] = input_data[col].fillna(input_data[col].mean())\n",
    "\n",
    "# Encode categorical features\n",
    "categorical_cols = input_data[expected_features].select_dtypes(include=['object']).columns\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    input_data[col] = le.fit_transform(input_data[col].astype(str))\n",
    "\n",
    "# -------------------------------\n",
    "# Step 3: Load Scaler and Transform Data\n",
    "# -------------------------------\n",
    "# Load the scaler used during training (adjust filename as needed)\n",
    "scaler = joblib.load('src/weight/scaler_sota.pkl')\n",
    "X_input = scaler.transform(input_data[expected_features])\n",
    "\n",
    "# -------------------------------\n",
    "# Step 4: Load Trained Model and Run Inference\n",
    "# -------------------------------\n",
    "# Load the saved model (adjust filename as needed)\n",
    "model = joblib.load('src/weight/xgb_model.pkl')\n",
    "\n",
    "# Predict probabilities for fraud class (assumed class 1 is fraud)\n",
    "fraud_probabilities = model.predict_proba(X_input)[:, 1]\n",
    "\n",
    "# Define fraud_score as the predicted fraud probability and confidence_score as 1 - fraud_score\n",
    "fraud_score = fraud_probabilities\n",
    "confidence_score = 1 - fraud_probabilities\n",
    "\n",
    "# Predict fraud label using a threshold of 0.5\n",
    "threshold = 0.5\n",
    "is_fraud_predicted = fraud_score >= threshold\n",
    "\n",
    "# -------------------------------\n",
    "# Step 5: Append Predictions to Data and Save as New CSV\n",
    "# -------------------------------\n",
    "input_data['is_fraud_predicted'] = is_fraud_predicted\n",
    "input_data['confidence_score'] = confidence_score\n",
    "input_data['fraud_score'] = fraud_score\n",
    "\n",
    "# Save the augmented CSV; adjust output filename as desired\n",
    "input_data.to_csv('src/output/output_with_predictions.csv', index=False)\n",
    "print(\"Output saved to 'output_with_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
